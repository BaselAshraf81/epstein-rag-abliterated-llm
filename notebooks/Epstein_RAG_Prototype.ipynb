{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eGHjuXX6S6e3"
      },
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "!pip install --upgrade pip\n",
        "!pip install llama-index-core llama-index-llms-groq llama-index-embeddings-huggingface llama-index-retrievers-bm25 llama-index-readers-file llama-index-vector-stores-faiss\n",
        "!pip install sentence-transformers faiss-cpu pandas transformers rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItRFd9BQb0qA"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjoVFyI095gs"
      },
      "outputs": [],
      "source": [
        "# Download Mistral-Nemo-12B Model\n",
        "import os\n",
        "\n",
        "model_name = \"Mistral-Nemo-Inst-2407-12B-Thinking-Uncensored-HERETIC-HI-Claude-Opus-Q4_K_M.gguf\"\n",
        "hf_filename = \"Mistral-Nemo-Inst-2407-12B-Thinking-Uncensored-HERETIC-HI-Claude-Opus.Q4_K_M.gguf\"\n",
        "drive_path = \"/content/drive/MyDrive/Colab_Notebooks/models/\"\n",
        "output_path = os.path.join(drive_path, model_name)\n",
        "model_url = f\"https://huggingface.co/mradermacher/Mistral-Nemo-Inst-2407-12B-Thinking-Uncensored-HERETIC-HI-Claude-Opus-GGUF/resolve/main/{hf_filename}\"\n",
        "\n",
        "if not os.path.exists(drive_path):\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "    print(f\"üìÇ Created directory: {drive_path}\")\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    print(f\"‚¨áÔ∏è Downloading {model_name}...\")\n",
        "    !wget -O \"{output_path}\" \"{model_url}\"\n",
        "    print(f\"‚úÖ Download complete: {output_path}\")\n",
        "else:\n",
        "    print(f\"‚è≠Ô∏è File already exists: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13ZQPtVOUC-F"
      },
      "outputs": [],
      "source": [
        "# Download Dataset\n",
        "#!wget https://huggingface.co/datasets/theelderemo/epstein-files-nov-2025/resolve/main/EPS_FILES_20K_NOV2025.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5deced0e"
      },
      "source": [
        "## Install llama-cpp-python with GPU Support\n",
        "\n",
        "**IMPORTANT:** Run this cell ONCE, then click \"RESTART RUNTIME\" when prompted. After restart, proceed to the next cell.\n",
        "\n",
        "**Do NOT run this cell again after restart.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NStvSwy_oe_A"
      },
      "outputs": [],
      "source": [
        "# Install llama-cpp-python with GPU Support\n",
        "import os\n",
        "import sys\n",
        "\n",
        "!pip uninstall -y numpy llama-cpp-python\n",
        "!pip install llama-index llama-index-embeddings-huggingface llama-index-retrievers-bm25\n",
        "!pip install llama-cpp-python \\\n",
        "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124 \\\n",
        "  --force-reinstall --upgrade --no-cache-dir\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è INSTALLATION COMPLETE.\")\n",
        "print(\"Please click 'RESTART RUNTIME' in the popup now (ONLY ONCE).\")\n",
        "print(\"After restarting, run the next cell.\")\n",
        "sys.exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a169718b"
      },
      "source": [
        "## Install LLM Integration\n",
        "\n",
        "Install the llama-cpp integration and reranker for LlamaIndex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dgYqVZ7CqVNw"
      },
      "outputs": [],
      "source": [
        "# Install LLM Integration\n",
        "!pip install llama-index-llms-llama-cpp\n",
        "!pip install llama-index-postprocessor-sbert-rerank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd501a3f"
      },
      "source": [
        "## Mount Drive for Main Script\n",
        "\n",
        "Mount Google Drive to access the pre-built index and model files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2s8j6NMiG3r"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "l0lqIeGlA3Z8"
      },
      "outputs": [],
      "source": [
        "#THIS IS THE MAIN SCRIPT WHICH LOADS AND RUNS EVERYTHING.\n",
        "# ============================================\n",
        "# STEP 1: NUCLEAR CLEANUP\n",
        "# Run this in a FRESH Colab runtime (Runtime > Restart runtime)\n",
        "# ============================================\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import shutil\n",
        "# Kill any existing CUDA contexts\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Hide GPU temporarily\n",
        "gc.collect()\n",
        "\n",
        "# Now make GPU visible again\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ FRESH START - GPU RESET COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION FLAGS\n",
        "# ============================================\n",
        "SKIP_EMBED_MODEL = False\n",
        "SKIP_LLM = False\n",
        "SKIP_INDEX = False\n",
        "SKIP_RETRIEVERS = False\n",
        "SKIP_QUERY_ENGINE = False\n",
        "\n",
        "print(\"\\nüîß Setting up uncensored chat system...\")\n",
        "print(f\"   Skip embed model: {SKIP_EMBED_MODEL}\")\n",
        "print(f\"   Skip LLM: {SKIP_LLM}\")\n",
        "print(f\"   Skip index: {SKIP_INDEX}\")\n",
        "print(f\"   Skip retrievers: {SKIP_RETRIEVERS}\")\n",
        "print(f\"   Skip query engine: {SKIP_QUERY_ENGINE}\")\n",
        "\n",
        "# ============================================\n",
        "# PATHS\n",
        "# ============================================\n",
        "DRIVE_INDEX_PATH = \"/content/drive/MyDrive/Colab_Notebooks/epstein_index_full\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Colab_Notebooks/models/Mistral-Nemo-Inst-2407-12B-Thinking-Uncensored-HERETIC-HI-Claude-Opus-Q4_K_M.gguf\"\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# LOAD LLM FIRST (BEFORE ANYTHING TOUCHES GPU!)\n",
        "# ============================================\n",
        "if not SKIP_LLM:\n",
        "    print(\"\\nü§ñ Loading LLM: MISTRAL-NEMO-12B THINKING ENGINE...\")\n",
        "    print(\"   ‚ö° Heretic Method: De-censored (14/100 refusal rate)\")\n",
        "    print(\"   üß† Thinking Engine: Self-reasoning before answering\")\n",
        "    print(\"   üéØ 12B Parameters: 50% larger than Qwen (8B)\")\n",
        "\n",
        "    from llama_index.llms.llama_cpp import LlamaCPP\n",
        "    from llama_index.core import Settings\n",
        "\n",
        "    # T4 VRAM breakdown: 15GB total\n",
        "    # - Model weights (Q4_K_M): ~7.6GB\n",
        "    # - KV cache scales with context (4.6GB @ 16k, 2.3GB @ 8k)\n",
        "    # - Compute buffer: ~2GB\n",
        "    # Safe maximum for T4: 8192 tokens (leaves headroom for compute)\n",
        "    EFFECTIVE_CONTEXT = 16368\n",
        "\n",
        "    llm = LlamaCPP(\n",
        "        model_path=MODEL_PATH,\n",
        "        # --- CRITICAL SAMPLER SETTINGS FOR RAG + THINKING ---\n",
        "        # FIXED: Changed from 0.3 to 0.7 per model card recommendations\n",
        "        temperature=0.1,           # Model card recommendation for thinking models\n",
        "        # FIXED: Increased from 2048 to 3072 to allow complete responses\n",
        "        max_new_tokens=2048,       # 200 words thinking + 800 words answer\n",
        "        context_window=EFFECTIVE_CONTEXT,\n",
        "        model_kwargs={\n",
        "            # --- FULL GPU POWER ---\n",
        "            \"n_gpu_layers\": -1,           # ALL layers to GPU\n",
        "            \"n_ctx\": EFFECTIVE_CONTEXT,   # 8k context window\n",
        "            \"n_batch\": 512,               # Larger batch for 12B model\n",
        "            \"n_ubatch\": 256,\n",
        "            \"f16_kv\": True,               # FP16 KV cache\n",
        "            \"offload_kqv\": True,          # Offload K/Q/V to GPU\n",
        "\n",
        "            # --- THINKING ENGINE SETTINGS (Model Card Recommendations) ---\n",
        "            # FIXED: Changed from 1.15 to 1.0 - CRITICAL FIX for infinite repetition\n",
        "            \"repeat_penalty\": 1.05,        # Model card: Use 1.0 for thinking models\n",
        "            \"top_k\": 40,                  # Model card: 40\n",
        "            \"top_p\": 0.95,                # Model card: 0.95\n",
        "            \"min_p\": 0.05,                # Model card: 0.05\n",
        "            \"mirostat\": 0,                # Disabled for RAG\n",
        "\n",
        "            # --- HARD STOPS ---\n",
        "            # FIXED: Removed \"\\n\\n\\n\" which was causing premature stopping\n",
        "            \"stop\": [\n",
        "                \"</s>\",\n",
        "                \"[INST]\",           # Stops hallucinating new user prompts\n",
        "                \"[/INST]\",\n",
        "                                    # Risky \"User:\",\n",
        "                \"<|im_end|>\",\n",
        "                \"<|endoftext|>\"\n",
        "            ],\n",
        "        },\n",
        "        verbose=True,\n",
        "        messages_to_prompt=None,\n",
        "    )\n",
        "    Settings.llm = llm\n",
        "    print(f\"‚úÖ MISTRAL-NEMO-12B THINKING LOADED\")\n",
        "    print(f\"   üß† Model: Mistral-Nemo-Inst-2407-12B-Thinking-Uncensored-HERETIC\")\n",
        "    print(f\"   üìä Size: 12B parameters (Q4_K_M ~7.6GB)\")\n",
        "    print(f\"   üéØ Context: {EFFECTIVE_CONTEXT} tokens\")\n",
        "    print(f\"   üí≠ Thinking: Self-reasoning blocks enabled\")\n",
        "    print(f\"   üîì Censorship: De-censored (14/100 refusal vs 87/100 base)\")\n",
        "    print(f\"   ‚ú® FIXED: temp=0.7, repeat_penalty=1.0, max_tokens=3072\")\n",
        "\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping LLM (already loaded)\")\n",
        "    try:\n",
        "        llm\n",
        "        print(\"   ‚úì llm found in memory\")\n",
        "    except NameError:\n",
        "        print(\"   ‚ö†Ô∏è  WARNING: llm not found! Set SKIP_LLM=False\")\n",
        "        SKIP_LLM = False\n",
        "\n",
        "# ============================================\n",
        "# NOW safe to import everything else\n",
        "# ============================================\n",
        "# NOTE: We do NOT hide GPU from PyTorch - that causes IndexError\n",
        "# Instead, we explicitly set embedding model to use CPU below\n",
        "\n",
        "print(\"\\nüì¶ Importing remaining dependencies...\")\n",
        "import torch\n",
        "from llama_index.core import StorageContext, load_index_from_storage, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "from llama_index.core.retrievers import QueryFusionRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank\n",
        "\n",
        "# ============================================\n",
        "# LOAD EMBEDDING MODEL\n",
        "# ============================================\n",
        "if not SKIP_EMBED_MODEL:\n",
        "    print(\"\\nüì¶ Loading embedding model...\")\n",
        "\n",
        "    embed_model = HuggingFaceEmbedding(\n",
        "        model_name=\"BAAI/bge-base-en-v1.5\",\n",
        "        device=\"cpu\"  # Keep embeddings on CPU to save GPU for LLM\n",
        "    )\n",
        "\n",
        "    Settings.embed_model = embed_model\n",
        "    print(\"‚úì Embeddings ready (bge-base-en-v1.5 on CPU)\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping embedding model (already loaded)\")\n",
        "    try:\n",
        "        embed_model\n",
        "        print(\"   ‚úì embed_model found in memory\")\n",
        "    except NameError:\n",
        "        print(\"   ‚ö†Ô∏è  WARNING: embed_model not found! Set SKIP_EMBED_MODEL=False\")\n",
        "        SKIP_EMBED_MODEL = False\n",
        "\n",
        "# ============================================\n",
        "# LOAD INDEX FROM GOOGLE DRIVE (OPTIMIZED)\n",
        "# ============================================\n",
        "if not SKIP_INDEX:\n",
        "    print(\"\\nüìÇ Loading index (optimized)...\")\n",
        "\n",
        "    import pickle\n",
        "    import shutil\n",
        "\n",
        "    LOCAL_INDEX_PATH = \"/content/epstein_index_local\"\n",
        "    PICKLE_PATH = \"/content/drive/MyDrive/Colab_Notebooks/epstein_index_full/epstein_index.pkl\"\n",
        "\n",
        "    # Strategy 1: Try pickle cache (fastest - ~5s)\n",
        "    if os.path.exists(PICKLE_PATH):\n",
        "        print(\"   Loading from pickle cache...\")\n",
        "        with open(PICKLE_PATH, 'rb') as f:\n",
        "            index = pickle.load(f)\n",
        "        print(\"   ‚úì Loaded from pickle (~5s)\")\n",
        "\n",
        "    # Strategy 2: Copy to local disk then load (first run - ~30s)\n",
        "    else:\n",
        "        print(\"   First run - copying from Google Drive to local disk...\")\n",
        "\n",
        "        if not os.path.exists(LOCAL_INDEX_PATH):\n",
        "            shutil.copytree(DRIVE_INDEX_PATH, LOCAL_INDEX_PATH)\n",
        "            print(\"   ‚úì Copied to local disk\")\n",
        "\n",
        "        print(\"   Loading from local disk...\")\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=LOCAL_INDEX_PATH)\n",
        "        index = load_index_from_storage(storage_context)\n",
        "\n",
        "        # Save pickle for next time\n",
        "        print(\"   Saving pickle cache for future runs...\")\n",
        "        with open(PICKLE_PATH, 'wb') as f:\n",
        "            pickle.dump(index, f)\n",
        "        print(\"   ‚úì Pickle saved (next load will be ~5s)\")\n",
        "\n",
        "    print(\"‚úì Index loaded (25,303 documents)\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping index (already loaded)\")\n",
        "    try:\n",
        "        index\n",
        "        print(\"   ‚úì index found in memory\")\n",
        "    except NameError:\n",
        "        print(\"   ‚ö†Ô∏è  WARNING: index not found! Set SKIP_INDEX=False\")\n",
        "        SKIP_INDEX = False\n",
        "\n",
        "# ============================================\n",
        "# SETUP HYBRID RETRIEVAL WITH CROSS-ENCODER\n",
        "# ============================================\n",
        "if not SKIP_RETRIEVERS:\n",
        "    print(\"\\nüîç Setting up optimized hybrid search with Cross-Encoder...\")\n",
        "\n",
        "    # 1. WIDEN THE NET: Fetch top 50 candidates from each method\n",
        "    # This gives the cross-encoder actual options to choose from.\n",
        "    INITIAL_TOP_K = 50\n",
        "\n",
        "    # Vector retriever (Semantic search)\n",
        "    vector_retriever = VectorIndexRetriever(\n",
        "        index=index,\n",
        "        similarity_top_k=INITIAL_TOP_K\n",
        "    )\n",
        "\n",
        "    # BM25 retriever (Keyword search)\n",
        "    bm25_retriever = BM25Retriever.from_defaults(\n",
        "        index=index,\n",
        "        similarity_top_k=INITIAL_TOP_K\n",
        "    )\n",
        "\n",
        "    # Hybrid fusion - combines both retrievers using reciprocal rank fusion\n",
        "    # This gives us diverse candidates for the cross-encoder to rerank\n",
        "    hybrid_retriever = QueryFusionRetriever(\n",
        "        retrievers=[vector_retriever, bm25_retriever],\n",
        "        similarity_top_k=INITIAL_TOP_K,\n",
        "        num_queries=1,\n",
        "        mode=\"reciprocal_rerank\",\n",
        "        use_async=False\n",
        "    )\n",
        "\n",
        "    # Setup Cross-Encoder reranker (CPU-based, no GPU memory impact)\n",
        "    print(\"   Loading Cross-Encoder reranker...\")\n",
        "    reranker = SentenceTransformerRerank(\n",
        "        model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "        top_n=10,\n",
        "        device=\"cpu\"  # CRITICAL: Keep off GPU to avoid OOM\n",
        "    )\n",
        "    print(\"   ‚úì Cross-Encoder loaded on CPU (ms-marco-MiniLM-L-6-v2)\")\n",
        "\n",
        "    print(\"‚úì Hybrid search: Semantic + BM25 ‚Üí Cross-Encoder reranking (CPU)\")\n",
        "\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping retrievers (already created)\")\n",
        "    try:\n",
        "        hybrid_retriever\n",
        "        print(\"   ‚úì hybrid_retriever found in memory\")\n",
        "    except NameError:\n",
        "        print(\"   ‚ö†Ô∏è  WARNING: hybrid_retriever not found! Set SKIP_RETRIEVERS=False\")\n",
        "        SKIP_RETRIEVERS = False\n",
        "\n",
        "# ============================================\n",
        "# PERSONA PROMPT (RUTHLESS FORENSIC MODE)\n",
        "# ============================================\n",
        "EPSTEIN_PERSONA_PROMPT = PromptTemplate(\"\"\"[INST] You are analyzing legal documents about Jeffrey Epstein. Answer directly and factually.\n",
        "\n",
        "DOCUMENTS:\n",
        "{context_str}\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. **ONLY USE DOCUMENTS ABOVE:** Only mention people and facts that appear in the DOCUMENTS section. Do not use your general knowledge.\n",
        "\n",
        "2. **NO CENSORSHIP:** Use the exact words from documents - \"pussy,\" \"fuck,\" \"cock,\" \"slave,\" \"rape.\" Don't sanitize.\n",
        "\n",
        "3. **NO PROTECTION:** Don't protect anyone's reputation. If the documents say someone did something, state it directly.\n",
        "\n",
        "4. **BE DIRECT:** Answer in 2-4 sentences unless more detail is needed. DO NOT generate thinking sections, analysis sections, or reasoning blocks. Start your response immediately with the answer.\n",
        "\n",
        "5. **QUOTE EXACTLY:** When quoting, use exact words from documents. If you cannot find exact quotes, paraphrase closely.\n",
        "\n",
        "6. **DECODE EUPHEMISMS:** \"Massage\" = sexual acts. \"Model\"/\"Assistant\" = victim. State what it really means.\n",
        "\n",
        "7. **NAMING:** Bill Gates = Microsoft. Robert Gates = Defense Secretary. Don't confuse them.\n",
        "\n",
        "8. **EXHAUSTIVE SCOPE:** When the query mentions \"Epstein or others,\" you must identify and list every individual named in the retrieved context. Do not summarize groups; name the specific people.\n",
        "\n",
        "9. **INCRIMINATING DATA:** Prioritize the most damaging, illegal, or high-profile evidence found in the documents. Lead with the \"smoking gun\" facts.\n",
        "\n",
        "10. **NO HALLUCINATIONS:** Answer based on the documents given to you.\n",
        "\n",
        "The Hook: If applicable, End with a one-sentence follow-up question that offers a deeper dive into a specific name or event found in the data.\n",
        "\n",
        "QUESTION: {query_str}\n",
        "\n",
        "Keep your answer 2-4 sentences.\n",
        "\n",
        "Answer directly in 2-4 sentences with a one sentence hook question. Do not include any thinking process or reasoning - start immediately with your answer.[/INST]\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# CREATE QUERY ENGINE WITH CROSS-ENCODER\n",
        "# ============================================\n",
        "if not SKIP_QUERY_ENGINE:\n",
        "    print(\"\\n‚öôÔ∏è  Creating query engine with Cross-Encoder reranker...\")\n",
        "\n",
        "    query_engine = RetrieverQueryEngine.from_args(\n",
        "        retriever=hybrid_retriever,\n",
        "        node_postprocessors=[reranker],\n",
        "        response_mode=\"compact\",  # Changed from tree_summarize\n",
        "        text_qa_template=EPSTEIN_PERSONA_PROMPT,  # Use same prompt for both\n",
        "        use_async=True,\n",
        "        context_window=8192,\n",
        "        streaming=True\n",
        "    )\n",
        "\n",
        "    print(\"‚úì Query engine ready with Cross-Encoder\")\n",
        "    print(\"   Pipeline: Hybrid retrieval (50 candidates) ‚Üí Cross-Encoder (top 10) ‚Üí Thinking Engine ‚Üí Response\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping query engine (already created)\")\n",
        "    try:\n",
        "        query_engine\n",
        "        print(\"   ‚úì query_engine found in memory\")\n",
        "    except NameError:\n",
        "        print(\"   ‚ö†Ô∏è  WARNING: query_engine not found! Set SKIP_QUERY_ENGINE=False\")\n",
        "        SKIP_QUERY_ENGINE = False\n",
        "\n",
        "# ============================================\n",
        "# CHAT FUNCTION\n",
        "# ============================================\n",
        "# ============================================\n",
        "# CHAT FUNCTION (WITH 90s HARD STOP)\n",
        "# ============================================\n",
        "# ============================================\n",
        "# CHAT FUNCTION (FIXED SCOPE)\n",
        "# ============================================\n",
        "def chat(question, show_sources=True, top_k=7, debug=False, show_thinking=False):\n",
        "    # 1. TELL PYTHON TO USE THE GLOBAL OBJECTS\n",
        "    global query_engine, reranker, hybrid_retriever, vector_retriever, bm25_retriever\n",
        "\n",
        "    # Update reranker's top_n\n",
        "    reranker.top_n = top_k\n",
        "\n",
        "    # Update retriever's top_k dynamically\n",
        "    hybrid_retriever.similarity_top_k = min(60, top_k * 6)\n",
        "\n",
        "    # Also update individual retrievers\n",
        "    for retriever in [vector_retriever, bm25_retriever]:\n",
        "        retriever.similarity_top_k = min(50, top_k * 5)\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"You: {question}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # Get Streaming Response\n",
        "        streaming_response = query_engine.query(question)\n",
        "\n",
        "        full_response_text = \"\"\n",
        "\n",
        "        # Iterate through the stream manually\n",
        "        try:\n",
        "            for token in streaming_response.response_gen:\n",
        "                # CHECK TIME: Stop if > 85 seconds\n",
        "                if time.time() - start_time > 85:\n",
        "                    print(\"\\n\\nüõë HARD TIME LIMIT REACHED (85s). STOPPING GENERATION.\")\n",
        "                    full_response_text += \"\\n[RESPONSE TRUNCATED: TIME LIMIT EXCEEDED]\"\n",
        "                    break\n",
        "\n",
        "                full_response_text += token\n",
        "\n",
        "        except Exception as stream_err:\n",
        "            print(f\"‚ö†Ô∏è Stream interruption: {stream_err}\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Reconstruct response object\n",
        "        class MockResponse:\n",
        "            def __init__(self, text, sources):\n",
        "                self.response = text\n",
        "                self.source_nodes = sources\n",
        "\n",
        "        response = MockResponse(full_response_text, streaming_response.source_nodes)\n",
        "\n",
        "        # --- ROBUST PARSING LOGIC ---\n",
        "        response_text = full_response_text.strip()\n",
        "        thinking_block = None\n",
        "        answer = response_text\n",
        "\n",
        "        # Case 1: Perfect formatting\n",
        "        if \"[[[thinking start]]]\" in response_text and \"[[[thinking end]]]\" in response_text:\n",
        "            try:\n",
        "                start_tag = \"[[[thinking start]]]\"\n",
        "                end_tag = \"[[[thinking end]]]\"\n",
        "                s_idx = response_text.index(start_tag) + len(start_tag)\n",
        "                e_idx = response_text.index(end_tag)\n",
        "                thinking_block = response_text[s_idx:e_idx].strip()\n",
        "                answer = response_text[e_idx + len(end_tag):].strip()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Case 2: Model forgot start tag but included end tag\n",
        "        elif \"[[[thinking end]]]\" in response_text:\n",
        "            try:\n",
        "                end_tag = \"[[[thinking end]]]\"\n",
        "                e_idx = response_text.index(end_tag)\n",
        "                thinking_block = response_text[:e_idx].strip()\n",
        "                answer = response_text[e_idx + len(end_tag):].strip()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # --- DISPLAY ---\n",
        "        if thinking_block and show_thinking:\n",
        "            print(\"üí≠ THINKING PROCESS:\")\n",
        "            print(\"-\" * 60)\n",
        "            print(thinking_block)\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        print(\"\\nüìù UNFILTERED REPORT:\")\n",
        "        print(answer if answer else \"(No answer generated - check context limits)\")\n",
        "\n",
        "        # Debug: Show prompt\n",
        "        if debug:\n",
        "            print(\"\\nüîç DEBUG - Prompt sent to model:\")\n",
        "            print(\"-\" * 60)\n",
        "            context = \"\\n\\n\".join([node.text[:200] + \"...\" for node in response.source_nodes])\n",
        "            debug_prompt = EPSTEIN_PERSONA_PROMPT.format(\n",
        "                context_str=context,\n",
        "                query_str=question\n",
        "            )\n",
        "            print(debug_prompt)\n",
        "            print(\"-\" * 60)\n",
        "            print()\n",
        "\n",
        "        print(f\"\\n‚è±Ô∏è  {elapsed:.2f}s (includes ~0.5-1s CPU Cross-Encoder + thinking time)\")\n",
        "\n",
        "        if show_sources:\n",
        "            print(f\"\\nüìÑ Referenced documents (Cross-Encoder ranked):\")\n",
        "            for i, node in enumerate(response.source_nodes[:top_k], 1):\n",
        "                print(f\"  {i}. {node.metadata['filename']}\")\n",
        "                snippet = node.text[:200].replace('\\n', ' ')\n",
        "                print(f\"     Preview: {snippet}...\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "# ============================================\n",
        "# READY\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üí¨ EPSTEIN DOCUMENTS - THINKING ENGINE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "‚úÖ MISTRAL-NEMO-12B THINKING ENGINE LOADED!\n",
        "\n",
        "Model: Mistral-Nemo-Inst-2407-12B-Thinking-Uncensored-HERETIC\n",
        "Base: Mistral Nemo 12B Instruct\n",
        "Fine-tuning: Claude Opus 4.5 High Reasoning data\n",
        "Context: 8,192 tokens (optimized for T4)\n",
        "GPU: All layers offloaded\n",
        "Documents: 25,303 from House Oversight Committee\n",
        "Search: Hybrid (Semantic + BM25) with Cross-Encoder Reranking\n",
        "\n",
        "üß† THINKING ENGINE:\n",
        "   ‚Ä¢ Self-reasoning blocks before answering\n",
        "   ‚Ä¢ Resolves ambiguities (e.g., Bill Gates vs Robert Gates)\n",
        "   ‚Ä¢ 12B parameters (50% larger than Qwen 8B)\n",
        "   ‚Ä¢ Higher logic capacity for complex queries\n",
        "\n",
        "üîì HERETIC STATS:\n",
        "   ‚Ä¢ Refusals: 14/100 (vs 87/100 base model)\n",
        "   ‚Ä¢ De-censored: Unfiltered, uncensored responses\n",
        "   ‚Ä¢ Reasoning: Compact 3-6 paragraph thinking blocks\n",
        "\n",
        "‚ú® CRITICAL FIXES APPLIED:\n",
        "   ‚Ä¢ temperature: 0.3 ‚Üí 0.7 (allows natural variation)\n",
        "   ‚Ä¢ repeat_penalty: 1.15 ‚Üí 1.0 (stops infinite loops)\n",
        "   ‚Ä¢ max_new_tokens: 2048 ‚Üí 3072 (complete responses)\n",
        "   ‚Ä¢ Removed \"\\n\\n\\n\" stop token (was causing premature stopping)\n",
        "   ‚Ä¢ Simplified prompt with clearer thinking block instructions\n",
        "\n",
        "üìÑ DOCUMENT SCOPE:\n",
        "This system searches the Jeffrey Epstein House Oversight Committee document release.\n",
        "Documents include information about:\n",
        "   ‚Ä¢ Jeffrey Epstein and his activities\n",
        "   ‚Ä¢ Associates and visitors (Bill Gates, Trump, Clinton, Prince Andrew, etc.)\n",
        "   ‚Ä¢ Flight logs, island visits, properties\n",
        "   ‚Ä¢ Allegations, depositions, legal documents\n",
        "   ‚Ä¢ Anyone mentioned in the investigation\n",
        "\n",
        "Options:\n",
        "  chat(\"Question\", show_sources=False)     # Hide sources\n",
        "  chat(\"Question\", show_thinking=True)     # Show reasoning process\n",
        "  chat(\"Question\", debug=True)             # Show full prompt\n",
        "  chat(\"Question\", top_k=15)               # More context documents\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#flask"
      ],
      "metadata": {
        "id": "jQwGBlz1Numy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "nCSGsEgfFOMM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request, jsonify, session, Response, stream_with_context\n",
        "from flask_cors import CORS\n",
        "import json\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "import os\n",
        "import time\n",
        "from collections import deque\n",
        "import secrets\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.secret_key = secrets.token_hex(32)\n",
        "CORS(app)\n",
        "\n",
        "# ============================================\n",
        "# THREAD SAFETY FOR LLM\n",
        "# ============================================\n",
        "log_lock = threading.Lock()  # Separate lock for file I/O\n",
        "\n",
        "# Producer-Consumer Queue for LLM requests\n",
        "request_queue = queue.Queue()\n",
        "results_store = {}  # Maps job_id -> result\n",
        "results_lock = threading.Lock()\n",
        "\n",
        "# ============================================\n",
        "# LOGGING SETUP\n",
        "# ============================================\n",
        "LOG_DIR = \"/content/drive/MyDrive/Colab_Notebooks/logs\"\n",
        "LOG_FILE = os.path.join(LOG_DIR, \"user_queries_log.jsonl\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "def log_query(session_id, query, response, response_time):\n",
        "    try:\n",
        "        # Truncate response if too long (max 10000 chars to prevent JSON issues)\n",
        "        truncated_response = response[:10000] if len(response) > 10000 else response\n",
        "        if len(response) > 10000:\n",
        "            truncated_response += \"\\n... [TRUNCATED]\"\n",
        "            print(f\"‚ö†Ô∏è Response truncated from {len(response)} to 10000 chars for logging\")\n",
        "\n",
        "        log_entry = {\n",
        "            \"timestamp\": datetime.now(datetime.UTC).isoformat() if hasattr(datetime, 'UTC') else datetime.utcnow().isoformat() + \"Z\",\n",
        "            \"session_id\": session_id,\n",
        "            \"query\": query,\n",
        "            \"response\": truncated_response,\n",
        "            \"response_time\": response_time\n",
        "        }\n",
        "\n",
        "        # Acquire lock before writing to prevent concurrent writes\n",
        "        with log_lock:\n",
        "            with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
        "                json_str = json.dumps(log_entry, ensure_ascii=False)\n",
        "                f.write(json_str + '\\n')\n",
        "                f.flush()  # Force write to disk immediately\n",
        "                os.fsync(f.fileno())  # Ensure OS writes to Google Drive\n",
        "                print(f\"‚úÖ Logged query to {LOG_FILE} ({len(json_str)} bytes)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Logging error: {type(e).__name__}: {e}\")\n",
        "        print(f\"   Query length: {len(query)}, Response length: {len(response)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ============================================\n",
        "# QUEUE & TIMING MANAGEMENT\n",
        "# ============================================\n",
        "class QueueManager:\n",
        "    def __init__(self, max_history=20):\n",
        "        self.processing_times = deque(maxlen=max_history)\n",
        "        self._active_requests = 0\n",
        "        self.total_processed = 0\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def add_time(self, duration):\n",
        "        self.processing_times.append(duration)\n",
        "        with self._lock:\n",
        "            self.total_processed += 1\n",
        "\n",
        "    def get_avg_time(self):\n",
        "        if not self.processing_times:\n",
        "            return 15.0\n",
        "        return sum(self.processing_times) / len(self.processing_times)\n",
        "\n",
        "    def estimate_wait(self):\n",
        "        with self._lock:\n",
        "            active = self._active_requests\n",
        "        if active <= 0:\n",
        "            return 0\n",
        "        avg_time = self.get_avg_time()\n",
        "        return avg_time * active\n",
        "\n",
        "    def increment_active(self):\n",
        "        with self._lock:\n",
        "            self._active_requests += 1\n",
        "\n",
        "    def decrement_active(self):\n",
        "        with self._lock:\n",
        "            self._active_requests -= 1\n",
        "            if self._active_requests < 0:\n",
        "                self._active_requests = 0\n",
        "\n",
        "    @property\n",
        "    def active_requests(self):\n",
        "        with self._lock:\n",
        "            return self._active_requests\n",
        "\n",
        "queue_mgr = QueueManager()\n",
        "\n",
        "# ============================================\n",
        "# WORKER THREAD FOR LLM PROCESSING\n",
        "# ============================================\n",
        "def llm_worker():\n",
        "    \"\"\"Single worker thread that processes LLM requests one at a time\"\"\"\n",
        "    print(\"üîß LLM worker thread started\")\n",
        "    while True:\n",
        "        try:\n",
        "            job = request_queue.get()\n",
        "            if job is None:  # Poison pill to stop worker\n",
        "                break\n",
        "\n",
        "            job_id = job['job_id']\n",
        "            message = job['message']\n",
        "            show_sources = job['show_sources']\n",
        "            show_thinking = job['show_thinking']\n",
        "            top_k = job['top_k']\n",
        "            debug = job['debug']\n",
        "            session_id = job['session_id']\n",
        "\n",
        "            print(f\"üîß Worker processing job {job_id[:8]}...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # Call chat function (no timeout here - let it complete)\n",
        "                response = chat(\n",
        "                    message,\n",
        "                    show_sources=show_sources,\n",
        "                    top_k=top_k,\n",
        "                    debug=debug,\n",
        "                    show_thinking=show_thinking\n",
        "                )\n",
        "\n",
        "                response_time = time.time() - start_time\n",
        "                queue_mgr.add_time(response_time)\n",
        "\n",
        "                response_text = response.response.strip()\n",
        "                thinking_block = None\n",
        "                answer = response_text\n",
        "\n",
        "                # Check for custom thinking tags\n",
        "                if \"[[[thinking start]]]\" in response_text and \"[[[thinking end]]]\" in response_text:\n",
        "                    try:\n",
        "                        start_tag = \"[[[thinking start]]]\"\n",
        "                        end_tag = \"[[[thinking end]]]\"\n",
        "                        s_idx = response_text.index(start_tag) + len(start_tag)\n",
        "                        e_idx = response_text.index(end_tag)\n",
        "                        thinking_block = response_text[s_idx:e_idx].strip()\n",
        "                        answer = response_text[e_idx + len(end_tag):].strip()\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                # Check for <think> tags\n",
        "                elif \"<think>\" in response_text and \"</think>\" in response_text:\n",
        "                    try:\n",
        "                        import re\n",
        "                        think_match = re.search(r'<think>(.*?)</think>', response_text, re.DOTALL)\n",
        "                        if think_match:\n",
        "                            thinking_block = think_match.group(1).strip()\n",
        "                            answer = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                sources = []\n",
        "                if show_sources and hasattr(response, 'source_nodes'):\n",
        "                    sources = [\n",
        "                        {\n",
        "                            'filename': node.metadata.get('filename', 'Unknown'),\n",
        "                            'index': i + 1\n",
        "                        }\n",
        "                        for i, node in enumerate(response.source_nodes[:top_k], 1)\n",
        "                    ]\n",
        "\n",
        "                log_query(session_id, message, answer, response_time)\n",
        "\n",
        "                result = {\n",
        "                    'success': True,\n",
        "                    'answer': answer,\n",
        "                    'thinking': thinking_block if show_thinking else None,\n",
        "                    'sources': sources if show_sources else None,\n",
        "                    'response_time': round(response_time, 2)\n",
        "                }\n",
        "\n",
        "                print(f\"‚úÖ Job {job_id[:8]} completed in {response_time:.2f}s\")\n",
        "\n",
        "            except Exception as e:\n",
        "                response_time = time.time() - start_time\n",
        "                error_msg = f\"Error: {type(e).__name__}: {str(e)}\"\n",
        "                log_query(session_id, message, error_msg, response_time)\n",
        "\n",
        "                print(f\"‚ùå Job {job_id[:8]} failed: {error_msg}\")\n",
        "\n",
        "                error_type = type(e).__name__\n",
        "                if \"OutOfMemoryError\" in error_type or \"CUDA out of memory\" in str(e):\n",
        "                    error_msg = \"‚ùå GPU out of memory. Wait a moment and try again.\"\n",
        "                    try:\n",
        "                        import torch\n",
        "                        torch.cuda.empty_cache()\n",
        "                        import gc\n",
        "                        gc.collect()\n",
        "                    except:\n",
        "                        pass\n",
        "                else:\n",
        "                    error_msg = f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "                result = {\n",
        "                    'success': False,\n",
        "                    'error': error_msg\n",
        "                }\n",
        "\n",
        "            # Store result\n",
        "            with results_lock:\n",
        "                results_store[job_id] = result\n",
        "\n",
        "            queue_mgr.decrement_active()\n",
        "            request_queue.task_done()\n",
        "\n",
        "        except Exception as worker_error:\n",
        "            print(f\"üî• Worker thread error: {worker_error}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# Start worker thread\n",
        "worker_thread = threading.Thread(target=llm_worker, daemon=True)\n",
        "worker_thread.start()\n",
        "\n",
        "# ============================================\n",
        "# ROUTES\n",
        "# ============================================\n",
        "@app.route('/')\n",
        "def index():\n",
        "    if 'session_id' not in session:\n",
        "        session['session_id'] = str(uuid.uuid4())\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/health')\n",
        "def health():\n",
        "    return jsonify({'status': 'ok', 'message': 'Server is running'})\n",
        "\n",
        "@app.route('/api/chat', methods=['POST'])\n",
        "def chat_endpoint():\n",
        "    print(\"üì® Received chat request\")\n",
        "    try:\n",
        "        data = request.json\n",
        "        print(f\"üìù Data: {data}\")\n",
        "        message = data.get('message', '').strip()\n",
        "        show_sources = data.get('show_sources', False)\n",
        "        show_thinking = data.get('show_thinking', False)\n",
        "        debug = data.get('debug', False)\n",
        "        top_k = data.get('top_k', 5)\n",
        "\n",
        "        session_id = session.get('session_id', str(uuid.uuid4()))\n",
        "        print(f\"üîë Session: {session_id[:8]}...\")\n",
        "\n",
        "        if not message or len(message) > 500:\n",
        "            print(\"‚ö†Ô∏è Invalid message length\")\n",
        "            return jsonify({\n",
        "                'error': '‚ö†Ô∏è Query too long. Maximum 500 characters.' if len(message) > 500 else 'Empty query',\n",
        "                'success': False\n",
        "            })\n",
        "\n",
        "        print(f\"üí¨ Processing: {message[:50]}...\")\n",
        "\n",
        "        # Create job\n",
        "        job_id = str(uuid.uuid4())\n",
        "        job = {\n",
        "            'job_id': job_id,\n",
        "            'message': message,\n",
        "            'show_sources': show_sources,\n",
        "            'show_thinking': show_thinking,\n",
        "            'top_k': top_k,\n",
        "            'debug': debug,\n",
        "            'session_id': session_id\n",
        "        }\n",
        "\n",
        "        # Add to queue\n",
        "        queue_mgr.increment_active()\n",
        "        request_queue.put(job)\n",
        "        print(f\"üì• Job {job_id[:8]} queued (queue size: {request_queue.qsize()})\")\n",
        "\n",
        "        # Wait for result with timeout (120 seconds to account for queue wait)\n",
        "        max_wait = 90\n",
        "        poll_interval = 0.5\n",
        "        elapsed = 0\n",
        "\n",
        "        while elapsed < max_wait:\n",
        "            with results_lock:\n",
        "                if job_id in results_store:\n",
        "                    result = results_store.pop(job_id)\n",
        "                    print(f\"üì§ Job {job_id[:8]} result retrieved\")\n",
        "                    return jsonify(result)\n",
        "\n",
        "            time.sleep(poll_interval)\n",
        "            elapsed += poll_interval\n",
        "\n",
        "        # Timeout - job still in queue or processing\n",
        "        # DON'T decrement here! Worker will handle it when job completes\n",
        "        print(f\"‚è±Ô∏è Job {job_id[:8]} timed out after {max_wait}s\")\n",
        "        return jsonify({\n",
        "            'success': False,\n",
        "            'error': '‚è±Ô∏è Request timed out. The server is busy, please try again.'\n",
        "        }), 504\n",
        "\n",
        "    except Exception as top_error:\n",
        "        print(f\"üî• Top-level error: {type(top_error).__name__}: {str(top_error)}\")\n",
        "        return jsonify({\n",
        "            'success': False,\n",
        "            'error': f\"Server error: {str(top_error)}\"\n",
        "        }), 500\n",
        "\n",
        "@app.route('/api/status', methods=['GET'])\n",
        "def status_endpoint():\n",
        "    return jsonify({\n",
        "        'active_requests': queue_mgr.active_requests,\n",
        "        'avg_response_time': round(queue_mgr.get_avg_time(), 2),\n",
        "        'total_processed': queue_mgr.total_processed\n",
        "    })\n",
        "\n",
        "@app.route('/api/status-stream')\n",
        "def status_stream():\n",
        "    \"\"\"Server-Sent Events endpoint for real-time status updates\"\"\"\n",
        "    def generate():\n",
        "        max_duration = 300  # 5 minutes max connection\n",
        "        start_time = time.time()\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Auto-disconnect after max_duration to prevent stale connections\n",
        "                if time.time() - start_time > max_duration:\n",
        "                    print(\"‚è±Ô∏è SSE connection timeout, closing gracefully\")\n",
        "                    break\n",
        "\n",
        "                status_data = {\n",
        "                    'active_requests': queue_mgr.active_requests,\n",
        "                    'avg_response_time': round(queue_mgr.get_avg_time(), 2),\n",
        "                    'total_processed': queue_mgr.total_processed\n",
        "                }\n",
        "                yield f\"data: {json.dumps(status_data)}\\n\\n\"\n",
        "                time.sleep(2)  # Update every 2 seconds\n",
        "            except GeneratorExit:\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è SSE error: {e}\")\n",
        "                break\n",
        "\n",
        "    return Response(\n",
        "        stream_with_context(generate()),\n",
        "        mimetype='text/event-stream',\n",
        "        headers={\n",
        "            'Cache-Control': 'no-cache',\n",
        "            'X-Accel-Buffering': 'no',\n",
        "            'Connection': 'keep-alive'\n",
        "        }\n",
        "    )\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\nüöÄ Starting Flask application...\")\n",
        "\n",
        "    # Import ngrok setup\n",
        "    from pyngrok import ngrok, conf\n",
        "    from google.colab import userdata\n",
        "\n",
        "    NGROK_AUTH_TOKEN = userdata.get('ngrok')\n",
        "    STATIC_DOMAIN = \"florentina-nonexternalized-marketta.ngrok-free.dev\"\n",
        "    PORT = 7860\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"üîå BINDING TO: https://{STATIC_DOMAIN}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "    ngrok.kill()\n",
        "\n",
        "    try:\n",
        "        url = ngrok.connect(PORT, domain=STATIC_DOMAIN).public_url\n",
        "        print(f\"‚úÖ SUCCESS! Your App is Live at: {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR: {e}\")\n",
        "        print(\"Falling back to random URL...\")\n",
        "        url = ngrok.connect(PORT).public_url\n",
        "        print(f\"‚ö†Ô∏è Temporary URL: {url}\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üåê Server starting on http://0.0.0.0:7860\")\n",
        "    print(\"üìù Logs will appear below...\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Use threaded mode and disable reloader for stability\n",
        "    app.run(host='0.0.0.0', port=PORT, debug=False, threaded=True, use_reloader=False)\n"
      ],
      "metadata": {
        "id": "XNr6ssBVGIPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c001eb3-d72a-4528-8799-c25f839358e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß LLM worker thread started\n",
            "üöÄ Starting Flask application...\n",
            "\n",
            "\n",
            "============================================================\n",
            "üîå BINDING TO: https://florentina-nonexternalized-marketta.ngrok-free.dev\n",
            "============================================================\n",
            "‚úÖ SUCCESS! Your App is Live at: https://florentina-nonexternalized-marketta.ngrok-free.dev\n",
            "============================================================\n",
            "üåê Server starting on http://0.0.0.0:7860\n",
            "üìù Logs will appear below...\n",
            "============================================================\n",
            "\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:7860\n",
            " * Running on http://172.28.0.12:7860\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Feb/2026 15:20:46] \"GET /api/status-stream HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Feb/2026 15:21:20] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Feb/2026 15:21:20] \"GET /static/style.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Feb/2026 15:21:20] \"GET /static/script.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Feb/2026 15:21:21] \"GET /api/status-stream HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Feb/2026 15:21:21] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è±Ô∏è SSE connection timeout, closing gracefully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/Feb/2026 15:25:52] \"GET /api/status-stream HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Feb/2026 15:40:13] \"GET /api/status-stream HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IU102RViKS-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "QOP5MApDNoxo"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}